{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d65641b7-d0b5-4e35-889d-e646150f1349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f832a933-45a0-456c-baa9-8b03fdec21be",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2=inputs[1]# our query is journey\n",
    "d_in=inputs.shape[1]\n",
    "d_out=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73428815-6b57-445c-9394-efcad434543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "w_q=torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
    "w_k=torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
    "w_v=torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b62df87-9713-40b9-9f69-e4b9f817bd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n"
     ]
    }
   ],
   "source": [
    "print(w_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7fc9a03-ada6-44e2-b9d4-80dc21841c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_2=x_2 @ w_k\n",
    "values_2=x_2 @ w_v\n",
    "query_2=x_2 @ w_q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374bbbf6-74ca-4c8b-b649-7461952c54b2",
   "metadata": {},
   "source": [
    "key=inputs @ w_k\n",
    "values=inputs @ w_v\n",
    "query=inputs @ w_q\n",
    "print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0078f348-3051-41c7-9526-d95e0e6fbc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3669, 0.7646],\n",
      "        [0.4433, 1.1419],\n",
      "        [0.4361, 1.1156],\n",
      "        [0.2408, 0.6706],\n",
      "        [0.1827, 0.3292],\n",
      "        [0.3275, 0.9642]])\n",
      "torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "key=inputs @ w_k\n",
    "values=inputs @ w_v\n",
    "query=inputs @ w_q\n",
    "print(key)\n",
    "print(query.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30093914-8ee9-4ad7-bab4-4a839608280a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# now we are focusing on the evaluating attention score\n",
    "attention_score_2=query_2 @ key.T\n",
    "print(attention_score_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58d8f851-2881-4244-a1ca-2f4b900e6ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
       "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
       "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
       "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
       "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
       "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores=query @ key.T\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14f0823c-76d5-481c-8946-02c2140a29bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1551, 0.2104, 0.2059, 0.1413, 0.1074, 0.1799],\n",
       "        [0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
       "        [0.1503, 0.2256, 0.2192, 0.1315, 0.0914, 0.1819],\n",
       "        [0.1591, 0.1994, 0.1962, 0.1477, 0.1206, 0.1769],\n",
       "        [0.1610, 0.1949, 0.1923, 0.1501, 0.1265, 0.1752],\n",
       "        [0.1557, 0.2092, 0.2048, 0.1419, 0.1089, 0.1794]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first we need to normalise the value to get attention weight\n",
    "d_k=key.shape[-1]\n",
    "att_weight=torch.softmax((attention_scores/d_k**0.5),dim=-1)\n",
    "att_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc7215f-4540-4af6-a8de-8533780d7c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##WHY DIVIDE BY SQRT (DIMENSION)\n",
    "Reason 1: For stability in learning\n",
    "\n",
    "The softmax function is sensitive to the magnitudes of its inputs. When the inputs are large, the differences between the exponential values of each input become much more pronounced. This causes the softmax output to become \"peaky,\" where the highest value receives almost all the probability mass, and the rest receive very little.\n",
    "\n",
    "In attention mechanisms, particularly in transformers, if the dot products between query and key vectors become too large (like multiplying by 8 in this example), the attention scores can become very large. This results in a very sharp softmax distribution, making the model overly confident in one particular \"key.\" Such sharp distributions can make learning unstable,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "61343f0a-e9b5-4cf3-a4d6-aaca84035a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalised valuestensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "normalized values tensor_8 tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "inputs=torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
    "softmax_result=torch.softmax(inputs,dim=-1)\n",
    "print(f\"normalised values{softmax_result}\")\n",
    "tensor_8=inputs*8\n",
    "softmax_result=torch.softmax(tensor_8,dim=-1)\n",
    "print(f\"normalized values tensor_8 {softmax_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05a447a-089b-4127-9165-622eea6eaf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BUT WHY SQRT?\n",
    "Reason 2: To make the variance of the dot product stable\n",
    "\n",
    "The dot product of Q and K increases the variance because multiplying two random numbers increases the variance.\n",
    "\n",
    "The increase in variance grows with the dimension.\n",
    "\n",
    "Dividing by sqrt (dimension) keeps the variance close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4273865d-aa9e-4d19-9eae-f41836445922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "987992c5-ca5a-45a0-a070-2149e2257e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance before scaling (dim=5): 4.804038701221977\n",
      "Variance after scaling (dim=5): 0.9608077402443952\n",
      "Variance before scaling (dim=100): 103.77816201692286\n",
      "Variance after scaling (dim=100): 1.0377816201692285\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to compute variance before and after scaling\n",
    "def compute_variance(dim, num_trials=1000):\n",
    "    dot_products = []\n",
    "    scaled_dot_products = []\n",
    "\n",
    "    # Generate multiple random vectors and compute dot products\n",
    "    for _ in range(num_trials):\n",
    "        q = np.random.randn(dim)\n",
    "        k = np.random.randn(dim)\n",
    "        \n",
    "        # Compute dot product\n",
    "        dot_product = np.dot(q, k)\n",
    "        dot_products.append(dot_product)\n",
    "        \n",
    "        # Scale the dot product by sqrt(dim)\n",
    "        scaled_dot_product = dot_product / np.sqrt(dim)\n",
    "        scaled_dot_products.append(scaled_dot_product)\n",
    "    \n",
    "    # Calculate variance of the dot products\n",
    "    variance_before_scaling = np.var(dot_products)\n",
    "    variance_after_scaling = np.var(scaled_dot_products)\n",
    "\n",
    "    return variance_before_scaling, variance_after_scaling\n",
    "\n",
    "# For dimension 5\n",
    "variance_before_5, variance_after_5 = compute_variance(5)\n",
    "print(f\"Variance before scaling (dim=5): {variance_before_5}\")\n",
    "print(f\"Variance after scaling (dim=5): {variance_after_5}\")\n",
    "\n",
    "# For dimension 20\n",
    "variance_before_100, variance_after_100 = compute_variance(100)\n",
    "print(f\"Variance before scaling (dim=100): {variance_before_100}\")\n",
    "print(f\"Variance after scaling (dim=100): {variance_after_100}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ece496a9-c41d-4e28-9d07-78ee9868a0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2996, 0.8053],\n",
       "        [0.3061, 0.8210],\n",
       "        [0.3058, 0.8203],\n",
       "        [0.2948, 0.7939],\n",
       "        [0.2927, 0.7891],\n",
       "        [0.2990, 0.8040]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector = att_weight @ values\n",
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eddecf66-0699-44a2-b94d-d626bf448325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class self_attention_v1:\n",
    "    def __init__(self,din,dout):\n",
    "        self.din=din\n",
    "        self.dout=dout\n",
    "        self.w_q=torch.nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.w_k=torch.nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.w_v=torch.nn.Parameter(torch.rand(d_in,d_out))\n",
    "    def forward(self,x):\n",
    "        keys=x @ self.w_k\n",
    "        values=x @self.w_v\n",
    "        query=x @ self.w_q\n",
    "        att_score=queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "18f1ee50-e7e5-42da-8e82-78ada2be7f2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x5 and 3x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m123\u001b[39m)\n\u001b[0;32m      2\u001b[0m sa_v1 \u001b[38;5;241m=\u001b[39mself_attention_v1(d_in, d_out)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(sa_v1\u001b[38;5;241m.\u001b[39mforward(inputs))\n",
      "Cell \u001b[1;32mIn[80], line 10\u001b[0m, in \u001b[0;36mself_attention_v1.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m---> 10\u001b[0m     keys\u001b[38;5;241m=\u001b[39mx \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_k\n\u001b[0;32m     11\u001b[0m     values\u001b[38;5;241m=\u001b[39mx \u001b[38;5;129m@self\u001b[39m\u001b[38;5;241m.\u001b[39mw_v\n\u001b[0;32m     12\u001b[0m     query\u001b[38;5;241m=\u001b[39mx \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_q\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x5 and 3x2)"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 =self_attention_v1(d_in, d_out)\n",
    "print(sa_v1.forward(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e778da19-ecab-4a7b-b903-86949dff248d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850ce3fc-3492-4f02-a66c-df688a1ef6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6945e2-2521-479e-a337-fe426bf445a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
